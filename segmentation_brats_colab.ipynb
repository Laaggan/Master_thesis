{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pc0e7NjANKvB"
   },
   "source": [
    "Linus code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4245,
     "status": "ok",
     "timestamp": 1571900254797,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "uDKIINKU_HFe",
    "outputId": "707becce-fda6-4c98-f133-0e2d16781638"
   },
   "outputs": [],
   "source": [
    "# Install package to be able to save keras weights\n",
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6781,
     "status": "ok",
     "timestamp": 1571900257347,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "L06f9hdTEJVq",
    "outputId": "2286fbc3-6317-44b3-af24-247834ff9f9f"
   },
   "outputs": [],
   "source": [
    "pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8960,
     "status": "ok",
     "timestamp": 1571900259551,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "5et-JeLXj3BG",
    "outputId": "276bd8e9-a060-4ea6-8195-11d998621178"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('my_drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17822,
     "status": "ok",
     "timestamp": 1571900268457,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "XN2r9qPvNKvJ",
    "outputId": "ba8ef823-4a05-4cae-bddb-a17e2e09c519"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import json\n",
    "import psutil\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "def print_memory_use():\n",
    "    '''\n",
    "    Function which prints current python memory usage\n",
    "    '''\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(process.memory_info().rss/1e9)\n",
    "\n",
    "# What value maps to what class\n",
    "mapping = {\n",
    "    0: \"Null class\",\n",
    "    1: \"Necrotic and non-enhancing tumor core\",\n",
    "    2: \"Edema\",\n",
    "    4: \"GD-enhancing tumor\"\n",
    "}\n",
    "\n",
    "mapping2 = {\n",
    "    0: \"Null class\",\n",
    "    1: \"Tumor\",\n",
    "}\n",
    "\n",
    "# fixme: skulle gå att göra bättre igenom att skicka med en tex tuple med titlarna\n",
    "# och returnera ett matplotlib-objekt istället för då hade man inte behövt ha olika\n",
    "# funktioner för \"plot_modalities\" och \"plt_OHE\" och också kunna ha två stycken figurer med \n",
    "# 2*2 subplots i en cell.\n",
    "def plot_modalities(x):\n",
    "    # Make sure input data is of correct shape\n",
    "    assert x.shape == (240, 240, 4), 'Shape of input data is incorrect'\n",
    "    \n",
    "    plt.subplot('221')\n",
    "    plt.imshow(x[:,:,0])\n",
    "    plt.axis('off')\n",
    "    plt.title('T1')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('222')\n",
    "    plt.imshow(x[:,:,1])\n",
    "    plt.axis('off')\n",
    "    plt.title('T1ce')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('223')\n",
    "    plt.imshow(x[:,:,2])\n",
    "    plt.axis('off')\n",
    "    plt.title('T2')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('224')\n",
    "    plt.imshow(x[:,:,3])\n",
    "    plt.axis('off')\n",
    "    plt.title('FLAIR')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_OHE(y):\n",
    "    # Make sure input data is of correct shape\n",
    "    assert y.shape == (240, 240, 4), 'Shape of input data is incorrect'\n",
    "    \n",
    "    plt.subplot('221')\n",
    "    plt.imshow(y[:,:,0])\n",
    "    plt.axis('off')\n",
    "    plt.title('Null')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('222')\n",
    "    plt.imshow(y[:,:,1])\n",
    "    plt.axis('off')\n",
    "    plt.title('\"Necrotic and non-enhancing tumor core\"')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('223')\n",
    "    plt.imshow(y[:,:,2])\n",
    "    plt.axis('off')\n",
    "    plt.title('Edema')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('224')\n",
    "    plt.imshow(y[:,:,3])\n",
    "    plt.axis('off')\n",
    "    plt.title('GD-enhancing tumor')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def shift_and_scale(x):\n",
    "    assert len(x.shape) == 2, 'The input must be 2 dimensional'\n",
    "    #assert np.std(x) != 0, 'Cant divide by zero'\n",
    "    result = x - np.mean(x)\n",
    "    \n",
    "    # This is a really ugly hack\n",
    "    if np.std(x) == 0:\n",
    "        result /= 1\n",
    "    else:\n",
    "        result /= np.std(x)\n",
    "    return result\n",
    "\n",
    "def OHE(Y, mapping):\n",
    "    '''\n",
    "    Takes in a picture as a matrix with labels and returns a one hot encoded tensor\n",
    "    \n",
    "    Parameters:\n",
    "    Y is the picture\n",
    "    Mapping is what value corresponds to what label\n",
    "    \n",
    "    Returns:\n",
    "    A tensor with a channel for each label.\n",
    "    '''\n",
    "    shape = Y.shape\n",
    "    labels = mapping.keys()\n",
    "    one_hot_enc = np.zeros(list(shape) + [len(labels)])\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        temp = np.zeros(shape)\n",
    "        ind = Y == label\n",
    "        temp[ind] = 1\n",
    "        one_hot_enc[:, :, i] = temp\n",
    "    return one_hot_enc\n",
    "\n",
    "def OHE2(Y):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    shape = Y.shape\n",
    "    # fixme: Having a OHE-encoding now is unnecessary\n",
    "    # 1 channels encodes all informations\n",
    "    one_hot_enc = np.zeros(list(shape) + [2])\n",
    "    temp = np.zeros(shape)\n",
    "    temp2 = np.ones(shape)\n",
    "    \n",
    "    ind1 = Y == 1\n",
    "    ind2 = Y == 4\n",
    "    temp[ind1] = 1\n",
    "    temp[ind2] = 1\n",
    "    \n",
    "    temp2 = temp2 - temp\n",
    "    \n",
    "    one_hot_enc[:, :, 0] = temp\n",
    "    one_hot_enc[:, :, 1] = temp2\n",
    "    return one_hot_enc\n",
    "\n",
    "def convert_brats(Y):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    shape = Y.shape\n",
    "    # fixme: Having a OHE-encoding now is unnecessary\n",
    "    # 1 channels encodes all informations\n",
    "    result = np.zeros(shape)\n",
    "    temp = np.zeros(shape)\n",
    "    \n",
    "    ind1 = Y == 1\n",
    "    ind2 = Y == 4\n",
    "    temp[ind1] = 1\n",
    "    temp[ind2] = 1\n",
    "    \n",
    "    result = temp\n",
    "    return result\n",
    "\n",
    "#fixme: I don't know if providing this mapping is necessary\n",
    "# probably could be provided inside function instead.\n",
    "def OHE_uncoding(y, mapping):\n",
    "    result = np.argmax(y, axis=2)\n",
    "    labels = mapping.keys()\n",
    "    temp = np.zeros(result.shape)\n",
    "    for i, label in enumerate(labels):\n",
    "        ind = result == i\n",
    "        temp[ind] = label\n",
    "    return temp\n",
    "\n",
    "def IoU_wholeTumor(y_true, y_pred):\n",
    "    values = np.array([0., 1.])\n",
    "    unique_y_pred = np.unique(y_pred)\n",
    "    unique_y_true = np.unique(y_true)\n",
    "    assert np.array_equal(y_pred.shape, y_true.shape), 'Prediction and ground truth must have same shape'\n",
    "    assert np.array_equal(values, unique_y_pred), 'yhat and y must be one hot encodings'\n",
    "    assert np.array_equal(values, unique_y_true), 'yhat and y must be one hot encodings'\n",
    "    \n",
    "    \n",
    "    y_pred[:,:,0] = np.logical_not(y_pred[:,:,0]) \n",
    "    y_true[:,:,0] = np.logical_not(y_true[:,:,0])\n",
    "    \n",
    "    intersection = np.logical_and(y_pred[:,:,0], y_true[:,:,0])\n",
    "    union = np.logical_or(y_true[:,:,0], y_pred[:,:,0])\n",
    "    \n",
    "    size_int = np.count_nonzero(intersection)\n",
    "    size_uni = np.count_nonzero(union)\n",
    "    \n",
    "    return size_int/size_uni\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(K.abs(y_true_f * y_pred_f), axis=-1)\n",
    "    return (2. * intersection) / (\n",
    "        K.sum(K.square(y_true_f), -1) + K.sum(K.square(y_pred_f), -1) + 1e-8)\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    return intersection/sum_\n",
    "\n",
    "def ful_IoU(y_true, y_pred):\n",
    "    intersection = K.cast(np.sum(K.eval(y_true)[:,0] == K.eval(y_pred)[:,0]), dtype='float32')\n",
    "    y1 = (K.eval(y_true)[:,0] > 0)\n",
    "    y2 = (K.eval(y_pred)[:,0] > 0)\n",
    "    union = K.cast(np.sum((y1 + y2) > 0), dtype='float32')\n",
    "    return intersection/union\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    values = np.array([0., 1.])\n",
    "    unique_y_pred = np.unique(y_pred)\n",
    "    unique_y_true = np.unique(y_true)\n",
    "    assert np.array_equal(y_pred.shape, y_true.shape), 'Prediction and ground truth must have same shape'\n",
    "    assert np.array_equal(values, unique_y_pred), 'yhat and y must be one hot encodings'\n",
    "    assert np.array_equal(values, unique_y_true), 'yhat and y must be one hot encodings'\n",
    "    \n",
    "    p_00 = np.count_nonzero(np.logical_and(y_pred[:,:,0], y_true[:,:,0]))\n",
    "    p_11 = np.count_nonzero(np.logical_and(y_pred[:,:,1], y_true[:,:,1]))\n",
    "    p_22 = np.count_nonzero(np.logical_and(y_pred[:,:,2], y_true[:,:,2]))\n",
    "    p_33 = np.count_nonzero(np.logical_and(y_pred[:,:,3], y_true[:,:,3]))\n",
    "\n",
    "    f_10 = np.count_nonzero(np.logical_and(y_pred[:,:,0], y_true[:,:,1]))\n",
    "    f_20 = np.count_nonzero(np.logical_and(y_pred[:,:,0], y_true[:,:,2]))\n",
    "    f_30 = np.count_nonzero(np.logical_and(y_pred[:,:,0], y_true[:,:,3]))\n",
    "\n",
    "    f_01 = np.count_nonzero(np.logical_and(y_pred[:,:,1], y_true[:,:,0]))\n",
    "    f_21 = np.count_nonzero(np.logical_and(y_pred[:,:,1], y_true[:,:,2]))\n",
    "    f_31 = np.count_nonzero(np.logical_and(y_pred[:,:,1], y_true[:,:,3]))\n",
    "\n",
    "    f_02 = np.count_nonzero(np.logical_and(y_pred[:,:,2], y_true[:,:,0]))\n",
    "    f_12 = np.count_nonzero(np.logical_and(y_pred[:,:,2], y_true[:,:,1]))\n",
    "    f_32 = np.count_nonzero(np.logical_and(y_pred[:,:,2], y_true[:,:,3]))\n",
    "\n",
    "    f_03 = np.count_nonzero(np.logical_and(y_pred[:,:,3], y_true[:,:,0]))\n",
    "    f_13 = np.count_nonzero(np.logical_and(y_pred[:,:,3], y_true[:,:,1]))\n",
    "    f_23 = np.count_nonzero(np.logical_and(y_pred[:,:,3], y_true[:,:,2]))\n",
    "\n",
    "    conf_matrix = np.array([p_00, f_01, f_02, f_03,\n",
    "                            f_10,p_11, f_12, f_13, \n",
    "                            f_20, f_21, p_22, f_23, \n",
    "                            f_30, f_31, f_32, p_33])\n",
    "    conf_matrix = conf_matrix.reshape(4,4)\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def reset_config(config, config_path=None, weights_path=None):\n",
    "    new_config = config\n",
    "    if weights_path:\n",
    "        assert type(weights_path) == str, 'The weight path must be a string'\n",
    "        new_config['weights_path'] = weights_path\n",
    "    if config_path:\n",
    "        assert type(config_path) == str, 'The config path must be a string'\n",
    "        new_config['config_path'] = config_path\n",
    "    new_config['history']['training_samples_used'] = 0\n",
    "    new_config['history']['loss'] = []\n",
    "    new_config['history']['val_loss'] = []\n",
    "    new_config['keep_training'] = False\n",
    "\n",
    "class CallbackJSON(Callback):\n",
    "    \"\"\" CallbackJSON descends from Callback\n",
    "        and is used to write the number of training samples that the model has been trained on\n",
    "        and the loss for a epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Save params in constructor\n",
    "        config: Is a dictionary loaded from a JSON file which is used to keep track of training\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.config_path = config['config_path']\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"\n",
    "        Updates the history of the config dict and saves it to a file\n",
    "        \"\"\"\n",
    "        # How many effective training samples have been used\n",
    "        self.config['history']['training_samples_used'] += self.config['samples_used']\n",
    "        \n",
    "        # Logs the loss of the current epoch\n",
    "        self.config['history']['loss'].append(logs['loss'])\n",
    "        #fixme: add the same code but for \"val_loss\"\n",
    "        self.config['history']['val_loss'].append(logs['val_loss'])\n",
    "        \n",
    "        print_memory_use()\n",
    "        # Save new config file\n",
    "        with open(self.config_path, \"w\") as f:\n",
    "            f.write(json.dumps(self.config))\n",
    "\n",
    "def load_patients(i, j, num_classes, base_path=\"\"):\n",
    "    assert j >= i, 'j>i has to be true, you have given an invalid range of patients.'\n",
    "    \n",
    "    path = base_path + \"MICCAI_BraTS_2019_Data_Training/*/*/*\"\n",
    "    \n",
    "    wild_t1 = path + \"_t1.nii.gz\"\n",
    "    wild_t1ce = path + \"_t1ce.nii.gz\"\n",
    "    wild_t2 = path + \"_t2.nii.gz\"\n",
    "    wild_flair = path + \"_flair.nii.gz\"\n",
    "    wild_gt = path + \"_seg.nii.gz\"\n",
    "    \n",
    "    t1_paths = glob.glob(wild_t1)\n",
    "    t1ce_paths = glob.glob(wild_t1ce)\n",
    "    t2_paths = glob.glob(wild_t2)\n",
    "    flair_paths = glob.glob(wild_flair)\n",
    "    gt_paths = glob.glob(wild_gt)\n",
    "\n",
    "    num_patients = j - i\n",
    "    ind = []\n",
    "    num_non_empty_slices = 0\n",
    "\n",
    "    for i in range(i, i + num_patients):\n",
    "        path_gt = gt_paths[i]\n",
    "        img_gt = nib.load(path_gt)\n",
    "        img_gt = img_gt.get_fdata()\n",
    "\n",
    "        curr_patient = []\n",
    "        # quick and dirty way to only get slices with tumor\n",
    "        for j in range(img_gt.shape[-1]):\n",
    "            if len(np.unique(img_gt[:,:,j])) >= num_classes:\n",
    "                curr_patient.append(j)\n",
    "                num_non_empty_slices += 1\n",
    "        ind.append(curr_patient)\n",
    "\n",
    "    image_data = np.zeros((4, 240, 240, num_non_empty_slices))\n",
    "    labels = np.zeros((num_non_empty_slices, 240, 240))\n",
    "    OHE_labels = np.zeros((num_non_empty_slices, 240, 240, 4))\n",
    "    next_ind = 0\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        print('Patient: ' + str(i))\n",
    "        curr_ind = ind[i]\n",
    "\n",
    "        path_t1 = t1_paths[i]\n",
    "        path_t1ce = t1ce_paths[i]\n",
    "        path_t2 = t2_paths[i]\n",
    "        path_flair = flair_paths[i]\n",
    "        path_gt = gt_paths[i]\n",
    "\n",
    "        img_t1 = nib.load(path_t1)\n",
    "        img_t1ce = nib.load(path_t1ce)\n",
    "        img_t2 = nib.load(path_t2)\n",
    "        img_flair = nib.load(path_flair)\n",
    "        img_gt = nib.load(path_gt)\n",
    "\n",
    "        img_t1 = img_t1.get_fdata()\n",
    "        img_t1ce = img_t1ce.get_fdata()\n",
    "        img_t2 = img_t2.get_fdata()\n",
    "        img_flair = img_flair.get_fdata()\n",
    "        img_gt = img_gt.get_fdata()\n",
    "\n",
    "        temp = 0\n",
    "        for i, x in enumerate(curr_ind):\n",
    "            image_data[0, :, :, next_ind + i] = img_t1[:,:,x]\n",
    "            image_data[1, :, :, next_ind + i] = img_t1ce[:,:,x]\n",
    "            image_data[2, :, :, next_ind + i] = img_t2[:,:,x]\n",
    "            image_data[3, :, :, next_ind + i] = img_flair[:,:,x]\n",
    "            labels[next_ind + i,:,:] = img_gt[:,:,x]\n",
    "            temp += 1\n",
    "        next_ind += temp\n",
    "\n",
    "    # I have here chosen to do shift and scale per image, \n",
    "    # which is not the only way to do normalization.\n",
    "    for j in range(num_non_empty_slices):\n",
    "        # shift and scale data\n",
    "        image_data[0, :, :, j] = shift_and_scale(image_data[0, :, :, j])\n",
    "        image_data[1, :, :, j] = shift_and_scale(image_data[1, :, :, j])\n",
    "        image_data[2, :, :, j] = shift_and_scale(image_data[2, :, :, j])\n",
    "        image_data[3, :, :, j] = shift_and_scale(image_data[3, :, :, j])\n",
    "\n",
    "        OHE_labels[j, :, :, :] = OHE(labels[j, :, :], mapping)\n",
    "\n",
    "    # The last axis will become the first axis\n",
    "    image_data = np.moveaxis(image_data, -1, 0)\n",
    "    image_data = np.moveaxis(image_data, 1, 3)\n",
    "    return (image_data, OHE_labels)\n",
    "\n",
    "def load_patients2(i, j, base_path=\"\"):\n",
    "    assert j >= i, 'j>i has to be true, you have given an invalid range of patients.'\n",
    "    \n",
    "    path = base_path + \"MICCAI_BraTS_2019_Data_Training/*/*/*\"\n",
    "    \n",
    "    wild_t1ce = path + \"_t1ce.nii.gz\"\n",
    "    wild_gt = path + \"_seg.nii.gz\"\n",
    "    \n",
    "    t1ce_paths = glob.glob(wild_t1ce)\n",
    "    gt_paths = glob.glob(wild_gt)\n",
    "\n",
    "    num_patients = j - i\n",
    "    ind = []\n",
    "    patients = set({})\n",
    "    num_non_empty_slices = 0\n",
    "    \n",
    "    for k in range(i, j):\n",
    "        path_gt = gt_paths[k]\n",
    "        img_gt = nib.load(path_gt)\n",
    "        img_gt = img_gt.get_fdata()\n",
    "        curr_patient = []\n",
    "        for l in range(img_gt.shape[-1]):\n",
    "            labels_in_slice = set(np.unique(img_gt[:,:,l]))\n",
    "            labels_of_interest = set([1,4])\n",
    "            if labels_of_interest.issubset(labels_in_slice):\n",
    "                curr_patient.append(l)\n",
    "                num_non_empty_slices += 1\n",
    "                patients.add(k)\n",
    "        ind.append(curr_patient)\n",
    "\n",
    "    image_data = np.zeros((1, 240, 240, num_non_empty_slices))\n",
    "    labels = np.zeros((num_non_empty_slices, 240, 240))\n",
    "    OHE_labels = np.zeros((num_non_empty_slices, 240, 240, 2))\n",
    "    next_ind = 0\n",
    "\n",
    "    for k, y in enumerate(patients):\n",
    "        print('Patient: ' + str(k))\n",
    "        curr_ind = ind[k]\n",
    "\n",
    "        path_t1ce = t1ce_paths[y]\n",
    "        path_gt = gt_paths[y]\n",
    "\n",
    "        img_t1ce = nib.load(path_t1ce)\n",
    "        img_gt = nib.load(path_gt)\n",
    "\n",
    "        img_t1ce = img_t1ce.get_fdata()\n",
    "        img_gt = img_gt.get_fdata()\n",
    "\n",
    "        temp = 0\n",
    "        for l, x in enumerate(curr_ind):\n",
    "            image_data[0, :, :, next_ind + l] = img_t1ce[:,:,x]\n",
    "            labels[next_ind + l,:,:] = img_gt[:,:,x]\n",
    "            temp += 1\n",
    "        next_ind += temp\n",
    "\n",
    "    for l in range(num_non_empty_slices):\n",
    "        image_data[0, :, :, l] = shift_and_scale(image_data[0, :, :, l])\n",
    "        OHE_labels[l, :, :, :] = OHE2(labels[l, :, :])\n",
    "\n",
    "    # The last axis will become the first axis\n",
    "    image_data = np.moveaxis(image_data, -1, 0)\n",
    "    image_data = np.moveaxis(image_data, 1, 3)\n",
    "    return (image_data, OHE_labels, patients)\n",
    "\n",
    "def load_patients3(i, j, base_path=\"\"):\n",
    "    assert j >= i, 'j>i has to be true, you have given an invalid range of patients.'\n",
    "    \n",
    "    path = base_path + \"MICCAI_BraTS_2019_Data_Training/*/*/*\"\n",
    "    \n",
    "    wild_t1ce = path + \"_t1ce.nii.gz\"\n",
    "    wild_gt = path + \"_seg.nii.gz\"\n",
    "    \n",
    "    t1ce_paths = glob.glob(wild_t1ce)\n",
    "    gt_paths = glob.glob(wild_gt)\n",
    "\n",
    "    num_patients = j - i\n",
    "    ind = []\n",
    "    num_non_empty_slices = 0\n",
    "\n",
    "    for k in range(i, i + num_patients):\n",
    "        path_gt = gt_paths[k]\n",
    "        img_gt = nib.load(path_gt)\n",
    "        img_gt = img_gt.get_fdata()\n",
    "\n",
    "        curr_patient = []\n",
    "        # Get slices containing class 1 and 4\n",
    "        for l in range(img_gt.shape[-1]):\n",
    "            labels_in_slice = set(np.unique(img_gt[:,:,l]))\n",
    "            labels_of_interest = set([1,4])\n",
    "            if labels_of_interest.issubset(labels_in_slice):\n",
    "                curr_patient.append(l)\n",
    "                num_non_empty_slices += 1\n",
    "        ind.append(curr_patient)\n",
    "\n",
    "    image_data = np.zeros((1, 240, 240, num_non_empty_slices))\n",
    "    labels = np.zeros((num_non_empty_slices, 240, 240))\n",
    "    next_ind = 0\n",
    "\n",
    "    for k in range(num_patients):\n",
    "        print('Patient: ' + str(k))\n",
    "        curr_ind = ind[k]\n",
    "\n",
    "        path_t1ce = t1ce_paths[k]\n",
    "        path_gt = gt_paths[k]\n",
    "\n",
    "        img_t1ce = nib.load(path_t1ce)\n",
    "        img_gt = nib.load(path_gt)\n",
    "\n",
    "        img_t1ce = img_t1ce.get_fdata()\n",
    "        img_gt = img_gt.get_fdata()\n",
    "\n",
    "        temp = 0\n",
    "        for l, x in enumerate(curr_ind):\n",
    "            image_data[0, :, :, next_ind + l] = img_t1ce[:,:,x]\n",
    "            labels[next_ind + l,:,:] = img_gt[:,:,x]\n",
    "            temp += 1\n",
    "        next_ind += temp\n",
    "\n",
    "    # I have here chosen to do shift and scale per image, \n",
    "    # which is not the only way to do normalization.\n",
    "    for l in range(num_non_empty_slices):\n",
    "        # shift and scale data\n",
    "        image_data[0, :, :, j] = shift_and_scale(image_data[0, :, :, l])\n",
    "        labels[l, :, :] = convert_brats(labels[l, :, :])\n",
    "\n",
    "    # The last axis will become the first axis\n",
    "    image_data = np.moveaxis(image_data, -1, 0)\n",
    "    image_data = np.moveaxis(image_data, 1, 3)\n",
    "    return (image_data, labels)\n",
    "\n",
    "def unet_binary(pretrained_weights = None, input_size = (256,256,1)):\n",
    "    inputs = Input(input_size)\n",
    "    #args = dict(activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = [IoU, dice_coefficient])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def unet(pretrained_weights = None, input_size = (256, 256, 1), num_classes=1, learning_rate=1e-4):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size = (2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size = (2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size = (2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size = (2, 2))(drop4)\n",
    "    \n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    \n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    \n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    \n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(num_classes, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    \n",
    "    reshape = Reshape((num_classes, input_size[0] * input_size[1]), input_shape = (num_classes, input_size[0], input_size[1]))(conv9)\n",
    "    permute = Permute((2, 1))(reshape)\n",
    "    activation = Softmax(axis=-1)(permute)\n",
    "    \n",
    "    model = Model(input = inputs, output = activation)\n",
    "    model.compile(optimizer = Adam(lr=learning_rate), loss = 'categorical_crossentropy', metrics=[dice_coefficient])\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    return model\n",
    "\n",
    "def unet_res(pretrained_weights = None, input_size = (256, 256, 4), num_classes=4, learning_rate=1e-4, dropout=0.5, this_activation='relu'):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = Conv2D(input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    merge1 = Add()([inputs, conv1])\n",
    "    merge1 = BatchNormalization()(merge1)\n",
    "    merge1 = Activation(this_activation)(merge1)\n",
    "    pool1 = MaxPooling2D(pool_size = (2, 2))(merge1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = Conv2D(input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    merge2 = Add()([pool1, conv2])\n",
    "    merge2 = BatchNormalization()(merge2)\n",
    "    merge2 = Activation(this_activation)(merge2)\n",
    "    pool2 = MaxPooling2D(pool_size = (2, 2))(merge2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = Conv2D(input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    merge3 = Add()([pool2, conv3])\n",
    "    merge3 = BatchNormalization()(merge3)\n",
    "    merge3 = Activation(this_activation)(merge3)\n",
    "    pool3 = MaxPooling2D(pool_size = (2, 2))(merge3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(dropout)(conv4)\n",
    "    drop4 = Conv2D(input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(drop4)\n",
    "    merge4 = Add()([pool3, drop4])\n",
    "    merge4 = BatchNormalization()(merge4)\n",
    "    merge4 = Activation(this_activation)(merge4)\n",
    "    pool4 = MaxPooling2D(pool_size = (2, 2))(merge4)\n",
    "    \n",
    "    conv5 = Conv2D(1024, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(dropout)(conv5)\n",
    "    drop5 = Conv2D(input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(drop5)\n",
    "    merge5 = Add()([pool4, drop5])\n",
    "    merge5 = BatchNormalization()(merge5)\n",
    "    merge5 = Activation(this_activation)(merge5)\n",
    "    \n",
    "    up6 = Conv2D(input_size[-1], 2, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(merge5))\n",
    "    merge6 = concatenate([merge4, up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6 = Conv2D(2*input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    merge6 = Add()([merge6, conv6])\n",
    "    merge6 = BatchNormalization()(merge6)\n",
    "    merge6 = Activation(this_activation)(merge6)\n",
    "    \n",
    "    up7 = Conv2D(input_size[-1], 2, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(merge6))\n",
    "    merge7 = concatenate([merge3, up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7 = Conv2D(2*input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    merge7 = Add()([merge7, conv7])\n",
    "    merge7 = BatchNormalization()(merge7)\n",
    "    merge7 = Activation(this_activation)(merge7)\n",
    "    \n",
    "    up8 = Conv2D(input_size[-1], 2, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(merge7))\n",
    "    merge8 = concatenate([merge2, up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8 = Conv2D(2*input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    merge8 = Add()([merge8, conv8])\n",
    "    merge8 = BatchNormalization()(merge8)\n",
    "    merge8 = Activation(this_activation)(merge8)\n",
    "    \n",
    "    up9 = Conv2D(input_size[-1], 2, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(merge8))\n",
    "    merge9 = concatenate([merge1, up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(num_classes, 3, activation = this_activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2*input_size[-1], 1, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    merge9 = Add()([merge9, conv9])\n",
    "    merge9 = BatchNormalization()(merge9)\n",
    "    merge9 = Activation(this_activation)(merge9)\n",
    "    \n",
    "    merge10 = Conv2D(input_size[-1], 1, activation = 'linear', padding ='same', kernel_initializer = 'he_normal')(merge9)\n",
    "    \n",
    "    reshape = Reshape((num_classes, input_size[0] * input_size[1]), input_shape = (num_classes, input_size[0], input_size[1]))(merge10)\n",
    "    permute = Permute((2, 1))(reshape)\n",
    "    activation = Softmax(axis=-1)(permute)\n",
    "    \n",
    "    model = Model(input = inputs, output = activation)\n",
    "    model.compile(optimizer = Adam(lr=learning_rate), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    return model\n",
    "\n",
    "def conv_block(input_, num_kernels, kernel_size, act_func, drop_rate):\n",
    "    conv = Conv2D(num_kernels, kernel_size,activation = act_func, padding = 'same', kernel_initializer = 'he_normal')(input_)\n",
    "    conv = Conv2D(num_kernels, kernel_size, activation = act_func, padding = 'same', kernel_initializer = 'he_normal')(conv)\n",
    "    drop = Dropout(drop_rate)(conv)\n",
    "    return conv\n",
    "\n",
    "def conv_block(input_, num_kernels, kernel_size, act_func, drop_rate):\n",
    "    argz = [num_kernels, kernel_size]\n",
    "    kwargz = {'activation':act_func, 'padding':'same', 'kernel_initializer':'he_normal'}\n",
    "    conv = Conv2D(*argz, **kwargz)(input_)\n",
    "    conv = Conv2D(*argz, **kwargz)(conv)\n",
    "    drop = Dropout(drop_rate)(conv)\n",
    "    return conv\n",
    "\n",
    "def conv_block_resnet(input_, num_kernels, kernel_size, act_func, drop_rate, input_size):\n",
    "    argz = [num_kernels, kernel_size]\n",
    "    kwargz = {'activation':act_func, 'padding':'same', 'kernel_initializer':'he_normal'}\n",
    "    conv = Conv2D(*argz, **kwargz)(input_)\n",
    "    conv = Conv2D(*argz, **kwargz)(conv)\n",
    "    conv = Conv2D(input_size[-1], (1,1), activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(conv)\n",
    "    conv = Dropout(drop_rate)(conv)\n",
    "    merge = Add()([input_, conv])\n",
    "    merge = BatchNormalization()(merge)\n",
    "    merge = Activation(act_func)(merge)\n",
    "    return merge\n",
    "\n",
    "def down_sampling_block(input_, act_func, num_kernels, drop_rate, input_size, res=False):\n",
    "    if res:\n",
    "        skip = conv_block_resnet(input_=input_, num_kernels=num_kernels, kernel_size=(3,3), \n",
    "                                 act_func=act_func, drop_rate=drop_rate, input_size=input_size)\n",
    "    else:\n",
    "        skip = conv_block(input_, num_kernels=num_kernels, kernel_size=(3,3), act_func=act_func, drop_rate=drop_rate)\n",
    "    pool = MaxPooling2D(pool_size = (2, 2))(skip)\n",
    "    return skip, pool\n",
    "\n",
    "def up_sampling_block(input_, skip, act_func, num_kernels, drop_rate, input_size, res=False):\n",
    "    up = UpSampling2D(size = (2, 2))(input_)\n",
    "    merge = concatenate([skip, up], axis = 3)\n",
    "    if res:\n",
    "        conv = conv_block_resnet(up, num_kernels=num_kernels, kernel_size=(3,3), \n",
    "                                 act_func=act_func, drop_rate=drop_rate, input_size=input_size)\n",
    "    else:\n",
    "        conv = conv_block(merge, num_kernels, (3,3), act_func, drop_rate)\n",
    "    return conv\n",
    "\n",
    "def unet_clean(pretrained_weights = None, input_size = (256, 256, 1), num_classes=2, learning_rate=1e-4, act_func='relu', res=False):\n",
    "    # Encoder\n",
    "    inputs = Input(input_size)\n",
    "    skip1, pool1 = down_sampling_block(inputs, act_func, num_kernels=64, drop_rate=0, input_size = input_size, res=res)\n",
    "    skip2, pool2 = down_sampling_block(pool1, act_func, num_kernels=128, drop_rate=0, input_size = input_size, res=res)\n",
    "    skip3, pool3 = down_sampling_block(pool2, act_func, num_kernels=256, drop_rate=0, input_size = input_size, res=res)\n",
    "    skip4, pool4 = down_sampling_block(pool3, act_func, num_kernels=512, drop_rate=0.2, input_size = input_size, res=res)\n",
    "    \n",
    "    #Bottleneck\n",
    "    conv5 = conv_block(pool4, 1024, 3, act_func, drop_rate=0.2)\n",
    "    \n",
    "    # Decoder\n",
    "    conv6 = up_sampling_block(conv5, skip4, act_func, 512, drop_rate = 0.2, input_size = input_size, res=res)\n",
    "    conv7 = up_sampling_block(conv6, skip3, act_func, 256, drop_rate = 0, input_size = input_size, res=res)\n",
    "    conv8 = up_sampling_block(conv7, skip2, act_func, 128, drop_rate = 0, input_size = input_size, res=res)\n",
    "    conv9 = up_sampling_block(conv8, skip1, act_func, 64, drop_rate = 0, input_size = input_size, res=res)\n",
    "    conv9 = Conv2D(num_classes, 1, activation = act_func, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    reshape = Reshape((num_classes, input_size[0] * input_size[1]), input_shape = (num_classes, input_size[0], input_size[1]))(conv9)\n",
    "    permute = Permute((2, 1))(reshape)\n",
    "    activation = Softmax(axis=-1)(permute)\n",
    "    \n",
    "    model = Model(input = inputs, output = activation)\n",
    "    model.compile(optimizer = Adam(lr=learning_rate), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    return model\n",
    "\n",
    "def unet_depth(pretrained_weights = None, input_size = (256, 256, 1), num_classes=2, learning_rate=1e-4, act_func='relu', res=False, \n",
    "               depth=4, num_kernels = [64, 128, 256, 512]):\n",
    "    assert depth == len(num_kernels), 'Depth and number of kernel sizes must be equal'\n",
    "    \n",
    "    encoder = []\n",
    "    inputs = Input(input_size)\n",
    "    for i in range(depth):\n",
    "        if i == 0:\n",
    "            skip, conv = down_sampling_block(inputs, act_func, num_kernels=num_kernels[i], drop_rate=0, input_size = input_size, res=res)\n",
    "            result = [skip, conv]\n",
    "            encoder.append(result)\n",
    "        else:\n",
    "            skip, conv = down_sampling_block(encoder[i-1][1], act_func, num_kernels=num_kernels[i], drop_rate=0, input_size = input_size, res=res)\n",
    "            result = [skip, conv]\n",
    "            encoder.append(result)\n",
    "\n",
    "    bottleneck = conv_block(encoder[depth - 1][1], 1024, 3, act_func, drop_rate=0.2)\n",
    "\n",
    "    decoder = []\n",
    "    for i in range(depth):\n",
    "        if i == 0:\n",
    "            skip = encoder[depth - 1][0]\n",
    "            decoder.append(up_sampling_block(bottleneck, skip, act_func, num_kernels=num_kernels[depth - i - 1], drop_rate=0, input_size = input_size, res=res))\n",
    "        else:\n",
    "            skip = encoder[depth - i - 1][0]\n",
    "            decoder.append(up_sampling_block(decoder[i - 1], skip, act_func, num_kernels=num_kernels[depth - i - 1], drop_rate=0, input_size = input_size, res=res))\n",
    "            \n",
    "    # prepare for softmax\n",
    "    conv = Conv2D(num_classes, 1, activation = act_func, padding = 'same', kernel_initializer = 'he_normal')(decoder[depth - 1])\n",
    "    reshape = Reshape((num_classes, input_size[0] * input_size[1]), input_shape = (num_classes, input_size[0], input_size[1]))(conv)\n",
    "    permute = Permute((2, 1))(reshape)\n",
    "    activation = Softmax(axis=-1)(permute)\n",
    "    \n",
    "    # Compile model and load pretrained weights\n",
    "    model = Model(input = inputs, output = activation)\n",
    "    model.compile(optimizer = Adam(lr=learning_rate), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    return model\n",
    "\n",
    "print('Finished')\n",
    "print_memory_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 321898,
     "status": "ok",
     "timestamp": 1571900572557,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "9lAteXmGZPqL",
    "outputId": "5a4c1e0c-0b03-4203-c40d-5d91bf7a445e"
   },
   "outputs": [],
   "source": [
    "#from my_lib import *\n",
    "from os import listdir\n",
    "import os\n",
    "\n",
    "# Set name of who is running the script to determine which path to use\n",
    "name = \"linus\"\n",
    "\n",
    "# Code snippet to fix that colab notebook and local notebook access data\n",
    "# through different paths\n",
    "var = os.uname()\n",
    "run_on_colab = var[0] == \"Linux\"\n",
    "\n",
    "carl_path = \"/content/my_drive/My Drive/Plugg/\"\n",
    "linus_path = \"/content/my_drive/My Drive/\"\n",
    "\n",
    "if name == \"linus\":\n",
    "  path = linus_path\n",
    "else:\n",
    "  path = carl_path\n",
    "\n",
    "if run_on_colab:\n",
    "    base_path = path + \"EXJOBB/\"\n",
    "else:\n",
    "    base_path = ''\n",
    "\n",
    "# Much cleaner loading of patients\n",
    "train_data = load_patients2(i=0, j=268, base_path=base_path)\n",
    "val_data = load_patients2(i=269, j=334, base_path=base_path)\n",
    "\n",
    "print('Finished')\n",
    "print_memory_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCUKXYxiZPqO"
   },
   "source": [
    "Separate input and labels and validate that the loading of the data has been done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 322733,
     "status": "ok",
     "timestamp": 1571900573402,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "PidEBsLpNKvT",
    "outputId": "0ec470eb-be35-48c0-b6dc-1ad0999119d2"
   },
   "outputs": [],
   "source": [
    "X_train = train_data[0]\n",
    "Y_train = train_data[1]\n",
    "\n",
    "X_val = val_data[0]\n",
    "Y_val = val_data[1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# This will show a slice of a patient\n",
    "ind = 0\n",
    "patient = X_train[ind, :, :, :]\n",
    "patient_labels = Y_train[ind, :, :]\n",
    "plt.imshow(patient[:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(patient_labels[:,:,0])\n",
    "plt.show()\n",
    "print(train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3705e3NYZ5ln"
   },
   "source": [
    "# New main training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 322717,
     "status": "ok",
     "timestamp": 1571900573403,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -60
    },
    "id": "8JHoeyzwFW9T",
    "outputId": "e17f795a-1cc1-4fd3-b4da-9f45045fbaf8"
   },
   "outputs": [],
   "source": [
    "# Load config file to session here\n",
    "if run_on_colab:\n",
    "    config_path = path + \"EXJOBB/training_sessions/trainin_tumor_slices_only/config.json\"\n",
    "    weights_path = path + \"EXJOBB/training_sessions/trainin_tumor_slices_only/weights.json\"\n",
    "    config_path = \"/content/my_drive/My Drive/EXJOBB/training_sessions/tumor_slices_with_all_classes/config.json\"\n",
    "else:\n",
    "    config_path = \"config_0.json\"\n",
    "\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Reset file to empty loss-values and/or change paths to config and weights\n",
    "weights_path = \"Training_session_0/weights.h5\"\n",
    "config_path = \"Training_session_0/config.json\"\n",
    "reset_config(config, config_path = config_path, weights_path=weights_path)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10416089,
     "status": "error",
     "timestamp": 1570728343243,
     "user": {
      "displayName": "Linus Lagergren",
      "photoUrl": "",
      "userId": "10069920663213268691"
     },
     "user_tz": -120
    },
    "id": "oFp9Vq4udUGo",
    "outputId": "e619def7-c222-44d2-cdbf-710499bcd746"
   },
   "outputs": [],
   "source": [
    "# The path to where to save weights and initialize ModelCheckpoint\n",
    "weights_path = config['weights_path']\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "MyModelCheckPoint = ModelCheckpoint(weights_path, verbose=0, save_weights_only=True, period=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "reset_config(config, config_path = config_path, weights_path=weights_path)\n",
    "\n",
    "if config['keep_training'] == True:\n",
    "    # Keep training on the old weights\n",
    "    my_unet = unet(input_size = (240, 240, 1), num_classes=2, pretrained_weights = weights_path)\n",
    "else:\n",
    "    # Initialize network\n",
    "    my_unet = unet(input_size = (240, 240, 1), num_classes=2)\n",
    "    config['keep_training'] = True\n",
    "\n",
    "assert not np.any(np.isnan(X_train)), 'Input contain nans'\n",
    "\n",
    "#Y_train = Y_train.reshape(Y_train.shape[0], -1, 4)\n",
    "validation_data = (X_val, Y_val.reshape(-1,240,240,1))\n",
    "\n",
    "#plt.imshow(my_unet.predict(X_train[60, :, :].reshape(1,240,240,1)).reshape(240,240))\n",
    "\n",
    "# Returns an object with accuracy and loss\n",
    "\n",
    "history = my_unet.fit(x=X_train, \n",
    "                      y=Y_train.reshape(-1,240,240,1), \n",
    "                      batch_size=64,\n",
    "                      epochs=100, \n",
    "                      verbose=1, \n",
    "                      callbacks=[CallbackJSON(config=config), MyModelCheckPoint, es],\n",
    "                      validation_split=0.0, \n",
    "                      validation_data=validation_data, \n",
    "                      shuffle=True, \n",
    "                      class_weight=None, \n",
    "                      sample_weight=None, \n",
    "                      initial_epoch=0, \n",
    "                      steps_per_epoch=None, \n",
    "                      validation_steps=None, \n",
    "                      validation_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfr5L5aFNKvc"
   },
   "outputs": [],
   "source": [
    "loss = config['history']['loss']\n",
    "val_loss = config['history']['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.show()\n",
    "plt.plot(val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEz-JB1iNKve"
   },
   "outputs": [],
   "source": [
    "ind = 10\n",
    "my_unet = unet(input_size = (240, 240, 4), num_classes = 4)\n",
    "my_unet.load_weights(config['weights_path'])\n",
    "\n",
    "yhat = my_unet.predict(val_data[ind, :, :, :].reshape(1, 240, 240, 4))\n",
    "\n",
    "#plot_OHE(yhat.reshape(240, 240, 4))\n",
    "\n",
    "plotable = OHE_uncoding(yhat.reshape(240, 240, 4), mapping)\n",
    "plt.imshow(plotable)\n",
    "plt.show()\n",
    "plt.imshow(val_labels[ind,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_r-v-APucCd"
   },
   "outputs": [],
   "source": [
    "config['weights_path'] = \"/content/my_drive/My Drive/EXJOBB/training_sessions/100_epochs_10_patients_lr_1e-4/weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eZr43sMNKvf"
   },
   "outputs": [],
   "source": [
    "my_unet = unet(input_size = (240, 240, 4), num_classes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOAHUXKPZPqc"
   },
   "outputs": [],
   "source": [
    "#yhat = my_unet.predict(X_train[48].reshape(1,240,240,4))\n",
    "yhat = my_unet.predict(X_train[0:100])\n",
    "#yhat = yhat.reshape(240,240,4)\n",
    "#plot_OHE(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eaoErIeZPqe"
   },
   "outputs": [],
   "source": [
    "\n",
    "hist = plt.hist(yhat[:,:,0].reshape(-1), bins='auto', log=True)\n",
    "plt.show()\n",
    "hist = plt.hist(yhat[:,:,1].reshape(-1), bins='auto', log=True)\n",
    "plt.show()\n",
    "hist = plt.hist(yhat[:,:,2].reshape(-1), bins='auto', log=True)\n",
    "plt.show()\n",
    "hist = plt.hist(yhat[:,:,3].reshape(-1), bins='auto', log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yDcqFWqZPqg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "segmentation_brats_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
