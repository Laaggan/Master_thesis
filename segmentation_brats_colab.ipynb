{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I change the text inside the raw text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "uDKIINKU_HFe",
    "outputId": "ebb20940-b721-4ca5-c7b2-12c3f8a1bc6f"
   },
   "outputs": [],
   "source": [
    "# Install package to be able to save keras weights\n",
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "L06f9hdTEJVq",
    "outputId": "3f7c28c4-d611-4b89-ccfb-ec076733f74f"
   },
   "outputs": [],
   "source": [
    "pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "5et-JeLXj3BG",
    "outputId": "93bf9c88-7cdb-43a4-af01-dc9978e792f0"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('my_drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_use():\n",
    "    '''\n",
    "    Function which prints current python memory usage\n",
    "    '''\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(process.memory_info().rss/1e9)\n",
    "\n",
    "# What value maps to what class\n",
    "mapping = {\n",
    "    0: \"Null class\",\n",
    "    1: \"Necrotic and non-enhancing tumor core\",\n",
    "    2: \"Edema\",\n",
    "    4: \"GD-enhancing tumor\"\n",
    "}\n",
    "\n",
    "# fixme: skulle gå att göra bättre igenom att skicka med en tex tuple med titlarna\n",
    "# och returnera ett matplotlib-objekt istället för då hade man inte behövt ha olika\n",
    "# funktioner för \"plot_modalities\" och \"plt_OHE\" och också kunna ha två stycken figurer med \n",
    "# 2*2 subplots i en cell.\n",
    "def plot_modalities(x):\n",
    "    # Make sure input data is of correct shape\n",
    "    assert x.shape == (240, 240, 4), 'Shape of input data is incorrect'\n",
    "    \n",
    "    plt.subplot('221')\n",
    "    plt.imshow(x[:,:,0])\n",
    "    plt.axis('off')\n",
    "    plt.title('T1')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('222')\n",
    "    plt.imshow(x[:,:,1])\n",
    "    plt.axis('off')\n",
    "    plt.title('T1ce')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('223')\n",
    "    plt.imshow(x[:,:,2])\n",
    "    plt.axis('off')\n",
    "    plt.title('T2')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('224')\n",
    "    plt.imshow(x[:,:,3])\n",
    "    plt.axis('off')\n",
    "    plt.title('FLAIR')\n",
    "    plt.colorbar()\n",
    "\n",
    "def plot_OHE(y):\n",
    "    # Make sure input data is of correct shape\n",
    "    assert y.shape == (240, 240, 4), 'Shape of input data is incorrect'\n",
    "    \n",
    "    plt.subplot('221')\n",
    "    plt.imshow(y[:,:,0])\n",
    "    plt.axis('off')\n",
    "    plt.title('Null')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('222')\n",
    "    plt.imshow(y[:,:,1])\n",
    "    plt.axis('off')\n",
    "    plt.title('\"Necrotic and non-enhancing tumor core\"')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('223')\n",
    "    plt.imshow(y[:,:,2])\n",
    "    plt.axis('off')\n",
    "    plt.title('Edema')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot('224')\n",
    "    plt.imshow(y[:,:,3])\n",
    "    plt.axis('off')\n",
    "    plt.title('GD-enhancing tumor')\n",
    "    plt.colorbar()\n",
    "\n",
    "def shift_and_scale(x):\n",
    "    assert len(x.shape) == 2, 'The input must be 2 dimensional'\n",
    "    #assert np.std(x) != 0, 'Cant divide by zero'\n",
    "    result = x - np.mean(x)\n",
    "    \n",
    "    # This is a really ugly hack\n",
    "    if np.std(x) == 0:\n",
    "      result /= 1\n",
    "    else:\n",
    "      result /= np.std(x)\n",
    "    return result\n",
    "\n",
    "def OHE(Y, mapping):\n",
    "    '''\n",
    "    Takes in a picture as a matrix with labels and returns a one hot encoded tensor\n",
    "    \n",
    "    Parameters:\n",
    "    Y is the picture\n",
    "    Mapping is what value corresponds to what label\n",
    "    \n",
    "    Returns:\n",
    "    A tensor with a channel for each label.\n",
    "    '''\n",
    "    shape = Y.shape\n",
    "    labels = mapping.keys()\n",
    "    one_hot_enc = np.zeros(list(shape) + [len(labels)])\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        temp = np.zeros(shape)\n",
    "        ind = Y == label\n",
    "        temp[ind] = 1\n",
    "        one_hot_enc[:, :, i] = temp\n",
    "    return one_hot_enc\n",
    "\n",
    "#fixme: I don't know if providing this mapping is necessary\n",
    "# probably could be provided inside function instead.\n",
    "def OHE_uncoding(y, mapping):\n",
    "    result = np.argmax(y, axis=2)\n",
    "    labels = mapping.keys()\n",
    "    temp = np.zeros(result.shape)\n",
    "    for i, label in enumerate(labels):\n",
    "        ind = result == i\n",
    "        temp[ind] = label\n",
    "    return temp\n",
    "\n",
    "def IoU_wholeTumor(y_true, y_pred):\n",
    "    values = np.array([0., 1.])\n",
    "    unique_y_pred = np.unique(y_pred)\n",
    "    unique_y_true = np.unique(y_true)\n",
    "    assert np.array_equal(y_pred.shape, y_true.shape), 'Prediction and ground truth must have same shape'\n",
    "    assert np.array_equal(values, unique_y_pred), 'yhat and y must be one hot encodings'\n",
    "    assert np.array_equal(values, unique_y_true), 'yhat and y must be one hot encodings'\n",
    "    \n",
    "    \n",
    "    y_pred[:,:,0] = np.logical_not(y_pred[:,:,0]) \n",
    "    y_true[:,:,0] = np.logical_not(y_true[:,:,0])\n",
    "    \n",
    "    intersection = np.logical_and(y_pred[:,:,0], y_true[:,:,0])\n",
    "    union = np.logical_or(y_true[:,:,0], y_pred[:,:,0])\n",
    "    \n",
    "    size_int = np.count_nonzero(intersection)\n",
    "    size_uni = np.count_nonzero(union)\n",
    "    \n",
    "    return size_int/size_uni\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(K.abs(y_true_f * y_pred_f), axis=-1)\n",
    "    return (2. * intersection) / (\n",
    "        K.sum(K.square(y_true_f), -1) + K.sum(K.square(y_pred_f), -1) + 1e-8)\n",
    "def reset_config(config, config_path=None, weights_path=None):\n",
    "    new_config = config\n",
    "    if weights_path:\n",
    "        assert type(weights_path) == str, 'The weight path must be a string'\n",
    "        new_config['weights_path'] = weights_path\n",
    "    if config_path:\n",
    "        assert type(config_path) == str, 'The config path must be a string'\n",
    "        new_config['config_path'] = config_path\n",
    "    new_config['history']['training_samples_used'] = 0\n",
    "    new_config['history']['loss'] = []\n",
    "    new_config['history']['val_loss'] = []\n",
    "    new_config['keep_training'] = False\n",
    "\n",
    "class CallbackJSON(Callback):\n",
    "    \"\"\" CallbackJSON descends from Callback\n",
    "        and is used to write the number of training samples that the model has been trained on\n",
    "        and the loss for a epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Save params in constructor\n",
    "        config: Is a dictionary loaded from a JSON file which is used to keep track of training\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.config_path = config['config_path']\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"\n",
    "        Updates the history of the config dict and saves it to a file\n",
    "        \"\"\"\n",
    "        # How many effective training samples have been used\n",
    "        self.config['history']['training_samples_used'] += self.config['samples_used']\n",
    "        \n",
    "        # Logs the loss of the current epoch\n",
    "        self.config['history']['loss'].append(logs['loss'])\n",
    "        #fixme: add the same code but for \"val_loss\"\n",
    "        #self.config['history']['val_loss'].append(logs['val_loss'])\n",
    "        \n",
    "        print_memory_use()\n",
    "        # Save new config file\n",
    "        with open(self.config_path, \"w\") as f:\n",
    "            f.write(json.dumps(self.config))\n",
    "\n",
    "print('Finished')\n",
    "print_memory_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQR3zTgY2Dnw"
   },
   "source": [
    "First part gets all paths to the different modalities and second part loads them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from os import listdir\n",
    "import os\n",
    "\n",
    "# Code snippet to fix that colab notebook and local notebook access data\n",
    "# through different paths\n",
    "var = os.uname()\n",
    "run_on_colab = var[0] == \"Linux\"\n",
    "if run_on_colab:\n",
    "    wild_t1 = \"/content/my_drive/My Drive/EXJOBB/MICCAI_BraTS_2019_Data_Training/*/*/*_t1.nii.gz\"\n",
    "    wild_t1ce = \"/content/my_drive/My Drive/EXJOBB/MICCAI_BraTS_2019_Data_Training/*/*/*_t1ce.nii.gz\"\n",
    "    wild_t2 = \"/content/my_drive/My Drive/EXJOBB/MICCAI_BraTS_2019_Data_Training/*/*/*_t2.nii.gz\"\n",
    "    wild_flair = \"/content/my_drive/My Drive/EXJOBB/MICCAI_BraTS_2019_Data_Training/*/*/*_flair.nii.gz\"\n",
    "    wild_gt = \"/content/my_drive/My Drive/EXJOBB/MICCAI_BraTS_2019_Data_Training/*/*/*_seg.nii.gz\"\n",
    "else:\n",
    "    wild_t1 = \"MICCAI_BraTS_2019_Data_Training/*/*/*_t1.nii.gz\"\n",
    "    wild_t1ce = \"MICCAI_BraTS_2019_Data_Training/*/*/*_t1ce.nii.gz\"\n",
    "    wild_t2 = \"MICCAI_BraTS_2019_Data_Training/*/*/*_t2.nii.gz\"\n",
    "    wild_flair = \"MICCAI_BraTS_2019_Data_Training/*/*/*_flair.nii.gz\"\n",
    "    wild_gt = \"MICCAI_BraTS_2019_Data_Training/*/*/*_seg.nii.gz\"\n",
    "\n",
    "t1_paths = glob.glob(wild_t1)\n",
    "t1ce_paths = glob.glob(wild_t1ce)\n",
    "t2_paths = glob.glob(wild_t2)\n",
    "flair_paths = glob.glob(wild_flair)\n",
    "gt_paths = glob.glob(wild_flair)\n",
    "\n",
    "num_patients = 5\n",
    "num_slices = 155\n",
    "org_shape = (4, 240, 240, 155)\n",
    "uncorrected_data = np.zeros((4, 240, 240, num_patients*num_slices))\n",
    "uncorrected_OHE_labels = np.zeros((num_patients*num_slices, 240, 240, 4))\n",
    "\n",
    "for i in range(num_patients):\n",
    "  print('Patient: ' + str(i))\n",
    "  path_t1 = t1_paths[i]\n",
    "  path_t1ce = t1ce_paths[i]\n",
    "  path_t2 = t2_paths[i]\n",
    "  path_flair = flair_paths[i]\n",
    "  path_gt = gt_paths[i]\n",
    "\n",
    "  img_t1 = nib.load(path_t1)\n",
    "  img_t1ce = nib.load(path_t1ce)\n",
    "  img_t2 = nib.load(path_t2)\n",
    "  img_flair = nib.load(path_flair)\n",
    "  img_gt = nib.load(path_gt)\n",
    "\n",
    "  img_t1 = img_t1.get_fdata()\n",
    "  img_t1ce = img_t1ce.get_fdata()\n",
    "  img_t2 = img_t2.get_fdata()\n",
    "  img_flair = img_flair.get_fdata()\n",
    "  img_gt = img_gt.get_fdata()\n",
    "\n",
    "  # I have here chosen to do shift and scale per image, \n",
    "  # which is not the only way to do normalization.\n",
    "  for j in range(img_t1.shape[2]):\n",
    "      # shift and scale data\n",
    "      img_t1[:, :, j] = shift_and_scale(img_t1[:, :, j])\n",
    "      img_t1ce[:, :, j] = shift_and_scale(img_t1ce[:, :, j])\n",
    "      img_t2[:, :, j] = shift_and_scale(img_t2[:, :, j])\n",
    "      img_flair[:, :, j] = shift_and_scale(img_flair[:, :, j])\n",
    "\n",
    "  assert not np.any(np.isnan(img_t1)), 'Inputs contain nans'\n",
    "  assert not np.any(np.isnan(img_t1ce)), 'Inputs contain nans'\n",
    "  assert not np.any(np.isnan(img_t2)), 'Inputs contain nans'\n",
    "  assert not np.any(np.isnan(img_flair)), 'Inputs contain nans'\n",
    "\n",
    "  uncorrected_data[0, :, :, i*num_slices:(i+1)*num_slices] = img_t1\n",
    "  uncorrected_data[1, :, :, i*num_slices:(i+1)*num_slices] = img_t1ce\n",
    "  uncorrected_data[2, :, :, i*num_slices:(i+1)*num_slices] = img_t2\n",
    "  uncorrected_data[3, :, :, i*num_slices:(i+1)*num_slices] = img_flair\n",
    "  \n",
    "  for j in range(num_slices):\n",
    "      uncorrected_OHE_labels[j, :, :, :] = OHE(img_gt[:, :, j], mapping)\n",
    "\n",
    "# The last axis will become the first axis\n",
    "uncorrected_data = np.moveaxis(uncorrected_data, -1, 0)\n",
    "uncorrected_data = np.moveaxis(uncorrected_data, 1, 3)\n",
    "\n",
    "print('Finished')\n",
    "print_memory_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHOIGg1Ubs1G"
   },
   "source": [
    "Load the next patient and use as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Yo7m4W3Matrn",
    "outputId": "995bbd24-700b-4829-b5b8-5f8081c8ca6b"
   },
   "outputs": [],
   "source": [
    "validation_index = num_patients\n",
    "\n",
    "validation_data = np.zeros((4, 240, 240, num_slices))\n",
    "validation_OHE_labels = np.zeros((num_slices, 240, 240, 4))\n",
    "\n",
    "# Pick one patient for validation\n",
    "path_t1 = t1_paths[validation_index]\n",
    "path_t1ce = t1ce_paths[validation_index]\n",
    "path_t2 = t2_paths[validation_index]\n",
    "path_flair = flair_paths[validation_index]\n",
    "path_gt = gt_paths[validation_index]\n",
    "\n",
    "img_t1 = nib.load(path_t1)\n",
    "img_t1ce = nib.load(path_t1ce)\n",
    "img_t2 = nib.load(path_t2)\n",
    "img_flair = nib.load(path_flair)\n",
    "img_gt = nib.load(path_gt)\n",
    "\n",
    "img_t1 = img_t1.get_fdata()\n",
    "img_t1ce = img_t1ce.get_fdata()\n",
    "img_t2 = img_t2.get_fdata()\n",
    "img_flair = img_flair.get_fdata()\n",
    "img_gt = img_gt.get_fdata()\n",
    "\n",
    "# I have here chosen to do shift and scale per image, \n",
    "# which is not the only way to do normalization.\n",
    "for j in range(img_t1.shape[2]):\n",
    "    # shift and scale data\n",
    "    img_t1[:, :, j] = shift_and_scale(img_t1[:, :, j])\n",
    "    img_t1ce[:, :, j] = shift_and_scale(img_t1ce[:, :, j])\n",
    "    img_t2[:, :, j] = shift_and_scale(img_t2[:, :, j])\n",
    "    img_flair[:, :, j] = shift_and_scale(img_flair[:, :, j])\n",
    "\n",
    "assert not np.any(np.isnan(img_t1)), 'Inputs contain nans'\n",
    "assert not np.any(np.isnan(img_t1ce)), 'Inputs contain nans'\n",
    "assert not np.any(np.isnan(img_t2)), 'Inputs contain nans'\n",
    "assert not np.any(np.isnan(img_flair)), 'Inputs contain nans'\n",
    "\n",
    "validation_data[0, :, :, :] = img_t1\n",
    "validation_data[1, :, :, :] = img_t1ce\n",
    "validation_data[2, :, :, :] = img_t2\n",
    "validation_data[3, :, :, :] = img_flair\n",
    "\n",
    "for j in range(num_slices):\n",
    "    validation_OHE_labels[j, :, :, :] = OHE(img_gt[:, :, j], mapping)\n",
    "\n",
    "# The last axis will become the first axis\n",
    "validation_data = np.moveaxis(validation_data, -1, 0)\n",
    "validation_data = np.moveaxis(validation_data, 1, 3)\n",
    "\n",
    "validation_OHE_labels = validation_OHE_labels.reshape(num_slices,-1,4)\n",
    "\n",
    "print('Finished')\n",
    "print_memory_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show a slice of a patient\n",
    "ind = 70\n",
    "patient = uncorrected_data[ind, :, :, :]\n",
    "print(patient.shape)\n",
    "plot_modalities(patient)\n",
    "\n",
    "for i in range(4):\n",
    "  print(\"Max:\", np.max(patient[:,:,i]), \"Min:\", np.min(patient[:,:,i]), sep=\" \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zkievcDbjPc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "def unet(pretrained_weights = None, input_size = (256, 256, 1), num_classes=1, learning_rate=1e-3):\n",
    "  inputs = Input(input_size)\n",
    "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "  pool1 = MaxPooling2D(pool_size = (2, 2))(conv1)\n",
    "  \n",
    "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "  pool2 = MaxPooling2D(pool_size = (2, 2))(conv2)\n",
    "  \n",
    "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "  pool3 = MaxPooling2D(pool_size = (2, 2))(conv3)\n",
    "  \n",
    "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "  drop4 = Dropout(0.5)(conv4)\n",
    "  pool4 = MaxPooling2D(pool_size = (2, 2))(drop4)\n",
    "  \n",
    "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "  drop5 = Dropout(0.5)(conv5)\n",
    "  \n",
    "  up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(drop5))\n",
    "  merge6 = concatenate([drop4, up6], axis = 3)\n",
    "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "  \n",
    "  up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv6))\n",
    "  merge7 = concatenate([conv3, up7], axis = 3)\n",
    "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "  \n",
    "  up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv7))\n",
    "  merge8 = concatenate([conv2, up8], axis = 3)\n",
    "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "  \n",
    "  up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2, 2))(conv8))\n",
    "  merge9 = concatenate([conv1, up9], axis = 3)\n",
    "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "  conv9 = Conv2D(num_classes, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "  \n",
    "  reshape = Reshape((num_classes, input_size[0] * input_size[1]), input_shape = (num_classes, input_size[0], input_size[1]))(conv9)\n",
    "  permute = Permute((2, 1))(reshape)\n",
    "  activation = Softmax(axis=-1)(permute)\n",
    "  \n",
    "  model = Model(input = inputs, output = activation)\n",
    "  model.compile(optimizer = Adam(lr=learning_rate), loss = 'categorical_crossentropy', metrics=[dice_coefficient])\n",
    "  if (pretrained_weights):\n",
    "    model.load_weights(pretrained_weights)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3705e3NYZ5ln"
   },
   "source": [
    "# New main training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "8JHoeyzwFW9T",
    "outputId": "1ae8f5cb-ebc5-4bf0-a397-0b4a3c0d94b2"
   },
   "outputs": [],
   "source": [
    "# Load config file to session here\n",
    "if run_on_colab:\n",
    "    config_path = \"/content/my_drive/My Drive/EXJOBB/training_sessions/Session_1/config_1.json\"\n",
    "else:\n",
    "    config_path = \"training_session_2/config_2.json\"\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "print(config)\n",
    "\n",
    "#print(reset_config.__code__.co_varnames)\n",
    "\n",
    "#reset_config(config, config_path = config_path, weights_path=\"training_session_2/weights_2.h5\")\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "oFp9Vq4udUGo",
    "outputId": "a48c9762-4060-4b82-adf0-3536e1a59855"
   },
   "outputs": [],
   "source": [
    "# The path to where to save weights and initialize ModelCheckpoint\n",
    "weights_path = config['weights_path']\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "MyModelCheckPoint = ModelCheckpoint(weights_path, verbose=0, save_weights_only=True)\n",
    "\n",
    "print(config['keep_training'])\n",
    "\n",
    "if config['keep_training'] == True:\n",
    "    # Keep training on the old weights\n",
    "    my_unet = unet(input_size = (240, 240, 4), num_classes = 4)\n",
    "    my_unet.load_weights(weights_path)\n",
    "else:\n",
    "    # Initialize network\n",
    "    my_unet = unet(input_size = (240, 240, 4), num_classes = 4)\n",
    "    config['keep_training'] = True\n",
    "    \n",
    "samples_used = 155\n",
    "# i = 70 has all labels present in the image\n",
    "n = uncorrected_data.shape[0]\n",
    "#i = np.random.randint(n, size=[samples_used,])\n",
    "i = [70]\n",
    "x = uncorrected_data[i, :, :, :]\n",
    "y = uncorrected_OHE_labels[i, :, :, :]\n",
    "y = y.reshape(len(i), -1, 4)\n",
    "\n",
    "print_memory_use()\n",
    "\n",
    "#del uncorrected_data\n",
    "#del uncorrected_OHE_labels\n",
    "\n",
    "print_memory_use()\n",
    "\n",
    "assert not np.any(np.isnan(x)), 'Input contain nans'\n",
    "\n",
    "# Returns an object with accuracy and loss\n",
    "history = my_unet.fit(x=x, \n",
    "                      y=y, \n",
    "                      batch_size=None,\n",
    "                      epochs=500, \n",
    "                      verbose=1, \n",
    "                      callbacks=[CallbackJSON(config=config), MyModelCheckPoint],\n",
    "                      validation_split=0.0, \n",
    "                      #validation_data=(validation_data, validation_OHE_labels), \n",
    "                      shuffle=True, \n",
    "                      class_weight=None, \n",
    "                      sample_weight=None, \n",
    "                      initial_epoch=0, \n",
    "                      steps_per_epoch=None, \n",
    "                      validation_steps=None, \n",
    "                      validation_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = config['history']['loss']\n",
    "\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_unet = unet(input_size = (240, 240, 4), num_classes = 4)\n",
    "my_unet.load_weights(config['weights_path'])\n",
    "\n",
    "yhat = my_unet.predict(uncorrected_data[70, :, :, :].reshape(1, 240, 240, 4))\n",
    "\n",
    "plot_OHE(yhat.reshape(240, 240, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "segmentation_brats_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
